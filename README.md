---
license: mit
title: 'Secure Internal Chatbot System: An AI-Powered Corporate Assistant with secured RAG Implementation and Role-Based Access Control'
sdk: docker
emoji: üè¢
colorFrom: green
colorTo: blue
short_description: A secure internal chatbot system for company employees
---
# üöÄ Secure Internal Chatbot System: An AI-Powered Corporate Assistant with secured RAG Implementation and Role-Based Access Control

This portal is a professional-grade internal chatbot system that processes natural language queries. It uses Retrieval-Augmented Generation (RAG) to provide context-rich, sourced responses restricted by role permissions.

## üõ†Ô∏è Getting Started (Local Setup)

Follow these steps to deploy the chatbot on your local machine using Docker.

### 1. Clone the Repository
```bash
# Clone the repository
git clone [https://github.com/springboardmentor1234x-stack/Internal-Chatbot-with-RBAC.git](https://github.com/springboardmentor1234x-stack/Internal-Chatbot-with-RBAC.git)

# Navigate into the project folder
cd Internal-Chatbot-with-RBAC
2. Build the Docker Image
This process installs dependencies and triggers data_ingest.py to index the corporate documents.

Bash

docker build -t internal-chatbot .
3. Run the Application
Bash

docker run -p 7860:7860 internal-chatbot
4. Access the UI
Open your browser and go to: http://localhost:7860

### üõ°Ô∏è Core Security: Role-Based Access Control (RBAC)
The system features a specialized security mechanism. The backend programmatically filters search results from the vector database based on your authenticated department.
1. **C-Level:** Full visibility into all corporate records.
2. **Departmental (HR, Finance, Engineering, Marketing):** Access restricted to their specific domain and general company policies.
3. **Employees:** Access limited to general handbooks and public documentation.

### üèóÔ∏è High-Performance Tech Stack
Built entirely on a free and open-source stack for maximum efficiency:

1. **LLM Intelligence:** Llama-3.1 (via Groq) for high-speed, context-aware responses.
2. **Vector Engine:** FAISS for sub-500ms semantic search retrieval.
3. **Embeddings:** HuggingFace `all-MiniLM-L6-v2` for lightweight text representation.
4. **Backend & UI:** FastAPI for secure RBAC middleware and Streamlit for a clean chat interface.
5. **Data Storage:** SQLite for user authentication and session management.


### ‚ö° Quick Implementation
1. **Ingest & Tag:** Documents are parsed, chunked, and tagged with departmental metadata.
2. **Index:** Embeddings are generated and stored in the FAISS vector database.
3. **Authenticate:** Users log in via a secure JWT-based portal.
4. **Query:** The RAG pipeline retrieves authorized context and generates cited, evidence-based answers.

This system guarantees zero unauthorized data access while delivering context-rich responses in under 3 seconds.
