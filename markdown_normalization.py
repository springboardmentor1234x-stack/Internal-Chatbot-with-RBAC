# -*- coding: utf-8 -*-
"""markdown_normalization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdE4_AP5t8rVGmWtd06b7-MY65cUiL0B
"""

file_path = "engineering_master_doc.md"

with open(file_path, "r", encoding="utf-8") as f:
    original_text = f.read()

print(original_text[:500])



import unicodedata
import re

def normalize_text(text):
    # Unicode normalization
    text = unicodedata.normalize("NFC", text)

    # Convert to lowercase
    text = text.lower()

    # Remove extra spaces and newlines
    text = re.sub(r"\s+", " ", text).strip()

    return text

cleaned_text = normalize_text(original_text)

print(cleaned_text[:500])



output_file = "cleaned_markdown.md"

with open(output_file, "w", encoding="utf-8") as f:
    f.write(cleaned_text)

print("Cleaned file saved successfully!")

comparison_file = "comparison_original_vs_cleaned.txt"

with open(comparison_file, "w", encoding="utf-8") as f:
    f.write(" ORIGINAL TEXT \n\n")
    f.write(original_text)
    f.write("\n\n CLEANED TEXT \n\n")
    f.write(cleaned_text)

print("Comparison file created successfully!")

import json
from datetime import datetime

# 1. Read the document
file_path = "engineering_master_doc.md"  # use your file name here

with open(file_path, "r", encoding="utf-8") as f:
    full_text = f.read()

print("Total characters in file:", len(full_text))

# 2. Simple tokenization (here: tokens = words)
tokens = full_text.split()
print("Total tokens (approx):", len(tokens))

CHUNK_SIZE = 350  # 350 tokens per chunk

chunks = []

# 3. Create chunks of 350 tokens each
for start_idx in range(0, len(tokens), CHUNK_SIZE):
    chunk_tokens = tokens[start_idx : start_idx + CHUNK_SIZE]
    chunk_text = " ".join(chunk_tokens)

    chunk_id = len(chunks)

    # 4. Metadata for this chunk
    metadata = {
        "chunk_id": chunk_id,
        "source_file": file_path,
        "token_start": start_idx,
        "token_end": start_idx + len(chunk_tokens) - 1,
        "chunk_size": len(chunk_tokens),
        "created_at_utc": datetime.utcnow().isoformat() + "Z",
    }

    chunks.append({
        "text": chunk_text,
        "metadata": metadata
    })

print("Total chunks created:", len(chunks))

# 5. Save all chunks into a new text file (for you / mentor)
output_text_file = "engineering_master_doc_chunks.txt"

with open(output_text_file, "w", encoding="utf-8") as f:
    for chunk in chunks:
        cid = chunk["metadata"]["chunk_id"]
        f.write(f"===== CHUNK {cid} =====\n")
        f.write(chunk["text"] + "\n\n")

print(f"All chunks written to: {output_text_file}")

# 6. Save metadata as JSON file (nice for RAG / later use)
metadata_file = "engineering_master_doc_chunks_metadata.json"

with open(metadata_file, "w", encoding="utf-8") as f:
    json.dump([c["metadata"] for c in chunks], f, indent=2)

print(f"Metadata saved to: {metadata_file}")

# 7. Show metadata for first 3 chunks (for assignment)
print("\nMetadata for first 3 chunks:\n")
for c in chunks[:3]:
    print(json.dumps(c["metadata"], indent=2))
    print("-" * 60)




from sentence_transformers import SentenceTransformer
import torch
import numpy as np


model_name = "sentence-transformers/all-MiniLM-L6-v2"
model = SentenceTransformer(model_name)

sentences = [

    "I love eating ice cream on weekends.",
    "Deep learning models can learn complex patterns from data."
]

print("Sentences:")
for i, s in enumerate(sentences):
    print(f"{i}: {s}")

embeddings = model.encode(sentences, convert_to_tensor=True)
print("\nEmbeddings shape:", embeddings.shape)  # e.g. (4, 384)


emb_norm = torch.nn.functional.normalize(embeddings, p=2, dim=1)


cosine_sim_matrix = torch.matmul(emb_norm, emb_norm.T)

print("\nCosine Similarity Matrix:")
print(cosine_sim_matrix)

def print_pairwise_sim(sentences, sim_matrix):
    n = len(sentences)
    print("\nPairwise Cosine Similarities:\n")
    for i in range(n):
        for j in range(i + 1, n):
            sim = sim_matrix[i, j].item()
            print(f"Sentence {i} â†” Sentence {j}: {sim:.4f}")

print_pairwise_sim(sentences, cosine_sim_matrix)


from sentence_transformers import SentenceTransformer
import torch

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

sentences = [
    "The company chatbot helps employees find documents.",
    "The internal chatbot retrieves company policies.",
    "Employees can take leave using the HR portal.",
    "Deep learning models require large datasets."
]

embeddings = model.encode(sentences, convert_to_tensor=True)

print("Embedding shape:", embeddings.shape)

normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

cosine_similarity = torch.matmul(normalized_embeddings, normalized_embeddings.T)

print("Cosine Similarity Matrix:\n")
print(cosine_similarity)


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

chunk_0 = """ # FinSolve Technologies Engineering Document ## 1. Introduction ### 1.1 Company Overview FinSolve Technologies is a leading FinTech company headquartered in Bangalore, India, with operations across North America, Europe, and Asia-Pacific. Founded in 2018, FinSolve provides innovative financial solutions, including digital banking, payment processing, wealth management, and enterprise financial analytics, serving over 2 million individual users and 10,000 businesses globally. ### 1.2 Purpose This engineering document outlines the technical architecture, development processes, and operational guidelines for FinSolve's product ecosystem. It serves as a comprehensive guide for engineering teams, stakeholders, and partners to ensure alignment with FinSolve's mission: "To empower financial freedom through secure, scalable, and innovative technology solutions."""
chunk_1 = """experience with biometric authentication, push notifications, and offline capabilities. * **Web Application**: A responsive Single Page Application (SPA) built with React, Redux, and Tailwind CSS, optimized for various screen sizes and compliant with WCAG 2.1 accessibility standards. * **API Interfaces**: RESTful and GraphQL APIs enabling third-party integrations, partner systems, and future expansions. #### 2.3.2 API Gateway * Centralized entry point for all client requests * Implements authentication, authorization, and rate limiting * Provides API versioning and documentation via Swagger/OpenAPI * Handles request logging and basic analytics"""
chunk_5 = """period with on-call support ## 5. Security and Compliance ### 5.1 Security Architecture #### 5.1.1 Authentication and Authorization * **User Authentication**: * OAuth 2.0 implementation with JWT tokens * Multi-factor authentication (MFA) via SMS, email, or authenticator apps * Biometric authentication for mobile devices * Session management with configurable timeouts * **Authorization**: * Role-Based Access Control (RBAC) for administrative functions * Attribute-Based Access Control (ABAC) for fine-grained permissions * Regular permission audits and least-privilege enforcement #### 5.1.2 Data Protection * **Encryption**: * Data in transit: TLS 1.3 for all communications * Data at rest: AES-256 encryption using AWS KMS"""

chunks = [chunk_0, chunk_1, chunk_5]


model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

embeddings = model.encode(chunks, convert_to_tensor=False)
print("Embedding shape:", embeddings.shape)          # (3, 384)

for i, emb in enumerate(embeddings):
    print(f"\nChunk {i} - first 10 values:")
    print(emb[:10])

sim_matrix = cosine_similarity(embeddings)
print("\nCosine similarity matrix:\n", sim_matrix)


import chromadb

client = chromadb.Client()


collection = client.create_collection(name="company_chunks")

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

chunk_texts = [
    "# FinSolve Technologies Engineering Document ## 1. Introduction ### 1.1 Company Overview FinSolve Technologies is a leading FinTech company headquartered in Bangalore, India, with operations across North America, Europe, and Asia-Pacific. Founded in 2018, FinSolve provides innovative financial solutions, including digital banking, payment processing, wealth management, and enterprise financial analytics, serving over 2 million individual users and 10,000 businesses globally. ### 1.2 Purpose This engineering document outlines the technical architecture, development processes, and operational guidelines for FinSolve's product ecosystem. It serves as a comprehensive guide for engineering teams, stakeholders, and partners to ensure alignment with FinSolve's mission: ",
    "experience with biometric authentication, push notifications, and offline capabilities. * **Web Application**: A responsive Single Page Application (SPA) built with React, Redux, and Tailwind CSS, optimized for various screen sizes and compliant with WCAG 2.1 accessibility standards. * **API Interfaces**: RESTful and GraphQL APIs enabling third-party integrations, partner systems, and future expansions. #### 2.3.2 API Gateway * Centralized entry point for all client requests * Implements authentication, authorization, and rate limiting * Provides API versioning and documentation via Swagger/OpenAPI * Handles request logging and basic analytics * AWS API Gateway with custom Lambda authorizers for sophisticated permission models #### 2.3.3 Microservices * **Authentication Service**:",
    "analytics and reporting workloads. * MongoDB sharding for user data distribution across multiple clusters. * Database connection pooling via PgBouncer to optimize connection management. #### 2.4.3 Caching Strategy * Multi-level caching architecture: * Application-level caching with Redis * API Gateway response caching * CDN caching for static assets * Database query result caching * Cache invalidation using event-based triggers and time-to-live (TTL) policies. ### 2.5 Resilience and Fault Tolerance #### 2.5.1 High Availability * Multi-Availability Zone (AZ) deployments in AWS regions. ",
]


embeddings = model.encode(chunk_texts)

print("Embedding shape:", embeddings.shape)



ids = ["chunk-0", "chunk-1", "chunk-2"]

metadatas = [
    {"department": "Engineering", "role": "Engineering"},
    {"department": "Engineering", "role": "C-Level"},
    {"department": "Finance", "role": "Finance"},
]

collection.add(
    ids=ids,
    embeddings=embeddings.tolist(),
    documents=chunk_texts,
    metadatas=metadatas,
)

print("Inserted", len(ids), "vectors into collection:", collection.name)



query_text = "How is the system architecture designed?"

results = collection.query(
    query_texts=[query_text],   # user query
    n_results=1                 # top 1 most similar chunk
)

print("Query:", query_text)
print("\nMost Relevant Chunk:\n")
print(results["documents"][0][0])

print("\nMetadata:\n")
print(results["metadatas"][0][0])



import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

client = chromadb.PersistentClient(path="chroma_db")

collection = client.get_or_create_collection(
    name="engineering_master"
)



chunk_texts = [
    "# FinSolve Technologies Engineering Document ## 1. Introduction ### 1.1 Company Overview FinSolve Technologies is a leading FinTech company headquartered in Bangalore, India, with operations across North America, Europe, and Asia-Pacific. Founded in 2018, FinSolve provides innovative financial solutions, including digital banking, payment processing, wealth management, and enterprise financial analytics, serving over 2 million individual users and 10,000 businesses globally. ### 1.2 Purpose This engineering document outlines the technical architecture, development processes, and operational guidelines for FinSolve's product ecosystem. It serves as a comprehensive guide for engineering teams, stakeholders, and partners to ensure alignment with FinSolve's mission: ",
    "experience with biometric authentication, push notifications, and offline capabilities. * **Web Application**: A responsive Single Page Application (SPA) built with React, Redux, and Tailwind CSS, optimized for various screen sizes and compliant with WCAG 2.1 accessibility standards. * **API Interfaces**: RESTful and GraphQL APIs enabling third-party integrations, partner systems, and future expansions. #### 2.3.2 API Gateway * Centralized entry point for all client requests * Implements authentication, authorization, and rate limiting * Provides API versioning and documentation via Swagger/OpenAPI * Handles request logging and basic analytics * AWS API Gateway with custom Lambda authorizers for sophisticated permission models #### 2.3.3 Microservices * **Authentication Service**:",
    "analytics and reporting workloads. * MongoDB sharding for user data distribution across multiple clusters. * Database connection pooling via PgBouncer to optimize connection management. #### 2.4.3 Caching Strategy * Multi-level caching architecture: * Application-level caching with Redis * API Gateway response caching * CDN caching for static assets * Database query result caching * Cache invalidation using event-based triggers and time-to-live (TTL) policies. ### 2.5 Resilience and Fault Tolerance #### 2.5.1 High Availability * Multi-Availability Zone (AZ) deployments in AWS regions. ",
]



from sentence_transformers import SentenceTransformer
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
embeddings = model.encode(chunk_texts)
print("Embedding shape:", embeddings.shape)

embedding_vectors = embeddings.tolist()
ids = ["chunk-0", "chunk-1", "chunk-2"]
collection.add(
    ids=ids,
    documents=chunk_texts,
    embeddings=embedding_vectors,
    metadatas=[
        {"department": "Engineering"},
        {"department": "Engineering"},
        {"department": "Engineering"}
    ]
)

from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
from sklearn.metrics.pairwise import cosine_similarity

chunks = [
    "Chunk 0: Our company follows Agile methodology with 2-week sprints and regular sprint reviews.",
    "Chunk 1: The HR department manages hiring, onboarding, and employee benefits.",
    "Chunk 2: Engineering team builds microservices using Python and FastAPI.",
    "Chunk 3: Finance department prepares quarterly financial reports and audits.",
    "Chunk 4: Marketing team handles social media campaigns and product promotions.",
    "Chunk 5: Security team enforces RBAC and MFA for all internal systems.",
    "Chunk 6: Customer support handles tickets using an internal helpdesk portal.",
    "Chunk 7: DevOps team manages CI/CD pipelines using GitHub Actions and Jenkins.",
    "Chunk 8: The company uses cloud infrastructure built on AWS and Kubernetes.",
    "Chunk 9: The employee handbook explains leave policies, code of conduct, and dress code.",
]

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")



embeddings = model.encode(chunks, convert_to_tensor=True)
print("Embedding shape:", embeddings.shape)


for i in range(3):
    print(f"\nChunk {i} - first 10 values:\n", embeddings[i][:10])

cos_matrix = cosine_similarity(embeddings)
print("\nCosine similarity matrix:\n", cos_matrix)

def my_knn(query, k=3):
    q_embed = model.encode([query], convert_to_tensor=True)
    sims = cosine_similarity(q_embed, embeddings)[0]
    top_k = sims.argsort()[::-1][:k]
    return [(int(i), float(sims[i])) for i in top_k]

print("\nQuery: 'Tell me about engineering'")
print(my_knn("Tell me about engineering", k=3))

print("\nQuery: 'Who manages financial reports?'")
print(my_knn("Who manages financial reports?", k=3))

import json

rbac_data = {
    "rules": {
        "deny_by_default": True,
        "admin_override": True
    },

    "role_hierarchy": {
        "Intern": [],
        "HR": ["Intern"],
        "Finance": ["Intern"],
        "Engineer": ["Intern"],
        "Manager": ["Finance", "Engineer"],
        "Admin": ["Manager"]
    },

    "roles_access": {
        "Intern": {
            "allowed_folders": ["general"]
        },
        "HR": {
            "allowed_folders": ["general", "HR"]
        },
        "Finance": {
            "allowed_folders": ["general", "Finance"]
        },
        "Engineer": {
            "allowed_folders": ["general", "Engineering"]
        },
        "Manager": {
            "allowed_folders": ["general", "Finance", "Engineering"]
        },
        "Admin": {
            "allowed_folders": ["*"]
        }
    },

    "documents": [
        {
            "doc_id": "DOC1",
            "folder": "general",
            "access": "all_roles"
        },
        {
            "doc_id": "DOC2",
            "folder": "HR",
            "access": ["HR", "Admin"]
        },
        {
            "doc_id": "DOC3",
            "folder": "Finance",
            "access": ["Finance", "Manager", "Admin"]
        },
        {
            "doc_id": "DOC4",
            "folder": "Engineering",
            "access": ["Engineer", "Manager", "Admin"]
        }
    ]
}

with open("rbac_config.json", "w") as f:
    json.dump(rbac_data, f, indent=4)
from google.colab import files
files.download("rbac_config.json")

import re

STOPWORDS = {"is", "the", "a", "an", "to", "of", "and", "for", "in", "what"}

def normalize_query(query):
    query = query.lower()
    query = re.sub(r"[^a-z0-9\s]", "", query)
    return query.strip()

def preprocess_query(query):
    query = normalize_query(query)
    words = query.split()
    words = [w for w in words if w not in STOPWORDS]
    return " ".join(words)

def rewrite_query(query):
    return preprocess_query(query)


def compute_effective_roles(user_roles, role_hierarchy):
    effective_roles = set(user_roles)
    stack = list(user_roles)

    while stack:
        role = stack.pop()
        for r in role_hierarchy.get(role, []):
            if r not in effective_roles:
                effective_roles.add(r)
                stack.append(r)

    return effective_roles


def rbac_filter_chunks(chunks, effective_roles):
    allowed_chunks = []

    for chunk in chunks:
        if chunk["access"] == "all_roles":
            allowed_chunks.append(chunk)
        elif effective_roles.intersection(set(chunk["access"])):
            allowed_chunks.append(chunk)

    return allowed_chunks

def knn_search(query, chunks, k=3):
    results = []
    q_tokens = set(query.split())

    for chunk in chunks:
        c_tokens = set(chunk["content"].split())
        score = len(q_tokens.intersection(c_tokens))
        results.append((chunk["doc_id"], score))

    results.sort(key=lambda x: x[1], reverse=True)
    return results[:k]


def run_pipeline(user_roles, query, role_hierarchy, chunks):
    effective_roles = compute_effective_roles(user_roles, role_hierarchy)
    rewritten_query = rewrite_query(query)
    allowed_chunks = rbac_filter_chunks(chunks, effective_roles)
    top_k = knn_search(rewritten_query, allowed_chunks)

    print("User Roles:", user_roles)
    print("Effective Roles:", effective_roles)
    print("Rewritten Query:", rewritten_query)
    print("Allowed Chunks:", len(allowed_chunks))
    print("Top K Results:", top_k)

role_hierarchy = {
    "Intern": [],
    "Finance": ["Intern"],
    "HR": ["Intern"],
    "Engineer": ["Intern"],
    "Manager": ["Finance", "Engineer"],
    "Admin": ["Manager"]
}

documents = [
    {"doc_id": "D1", "access": "all_roles", "content": "company policy"},
    {"doc_id": "D2", "access": ["Finance"], "content": "salary finance report"},
    {"doc_id": "D3", "access": ["HR"], "content": "sick leave policy"},
    {"doc_id": "D4", "access": ["Engineer"], "content": "system design document"}
]


def knn_search(query, chunks, k=3):
    results = []
    q_tokens = set(query.split())

    for chunk in chunks:
        c_tokens = set(chunk["content"].split())
        score = len(q_tokens.intersection(c_tokens))
        results.append((chunk["doc_id"], score))

    results.sort(key=lambda x: x[1], reverse=True)
    return results[:k]


def run_pipeline(user_roles, query, role_hierarchy, chunks):
    effective_roles = compute_effective_roles(user_roles, role_hierarchy)
    rewritten_query = rewrite_query(query)
    allowed_chunks = rbac_filter_chunks(chunks, effective_roles)
    top_k = knn_search(rewritten_query, allowed_chunks)

    print("User Roles:", user_roles)
    print("Effective Roles:", effective_roles)
    print("Rewritten Query:", rewritten_query)
    print("Allowed Chunks:", len(allowed_chunks))
    print("Top K Results:", top_k)

run_pipeline(
    user_roles=["Manager"],
    query="What is salary and system design?",
    role_hierarchy=role_hierarchy,
    chunks=documents
)