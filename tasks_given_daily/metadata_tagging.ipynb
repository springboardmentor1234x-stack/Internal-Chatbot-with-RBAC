{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "# ----------------------------\n",
        "# Function to split text into chunks\n",
        "# ----------------------------\n",
        "def create_chunks(text, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Splits text into chunks of approximately chunk_size words.\n",
        "    Returns a list of dictionaries with chunk text and start/end positions.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start_word_idx = 0\n",
        "\n",
        "    while start_word_idx < len(words):\n",
        "        chunk_words = words[start_word_idx:start_word_idx + chunk_size]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "        chunks.append({\n",
        "            'text': chunk_text,\n",
        "            'start_word_idx': start_word_idx + 1,\n",
        "            'end_word_idx': start_word_idx + len(chunk_words)\n",
        "        })\n",
        "        start_word_idx += chunk_size\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# ----------------------------\n",
        "# Function to create metadata for each chunk\n",
        "# ----------------------------\n",
        "def create_metadata(chunks, file_name):\n",
        "    \"\"\"\n",
        "    Takes chunk data and returns metadata list.\n",
        "    \"\"\"\n",
        "    metadata = []\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        meta = {\n",
        "            'chunk_id': idx + 1,\n",
        "            'file_name': file_name,\n",
        "            'num_words': len(chunk['text'].split()),\n",
        "            'start_word_idx': chunk['start_word_idx'],\n",
        "            'end_word_idx': chunk['end_word_idx'],\n",
        "            'char_count': len(chunk['text']),\n",
        "            'summary': '',  # placeholder for summary\n",
        "            'text': chunk['text']\n",
        "        }\n",
        "        metadata.append(meta)\n",
        "    return metadata\n",
        "\n",
        "# ----------------------------\n",
        "# Main Script\n",
        "# ----------------------------\n",
        "\n",
        "# Input document file\n",
        "input_file = 'large_document.txt'  # replace with your file path\n",
        "\n",
        "# Read document\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create chunks\n",
        "chunk_size = 500  # number of words per chunk\n",
        "chunks = create_chunks(text, chunk_size)\n",
        "\n",
        "# Generate metadata\n",
        "metadata = create_metadata(chunks, os.path.basename(input_file))\n",
        "\n",
        "# Save metadata and chunks to CSV\n",
        "output_file = 'document_chunks_metadata.csv'\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['chunk_id', 'file_name', 'num_words', 'start_word_idx', 'end_word_idx', 'char_count', 'summary', 'text']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for data in metadata:\n",
        "        writer.writerow(data)\n",
        "\n",
        "print(f\"\\nChunks and metadata saved to {output_file}\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# Print first few chunks to console\n",
        "# ----------------------------\n",
        "print(\"Preview of first 3 chunks:\\n\")\n",
        "for data in metadata[:3]:\n",
        "    print(f\"Chunk ID: {data['chunk_id']}\")\n",
        "    print(f\"Number of words: {data['num_words']}\")\n",
        "    print(f\"Start word index: {data['start_word_idx']}\")\n",
        "    print(f\"End word index: {data['end_word_idx']}\")\n",
        "    print(f\"Character count: {data['char_count']}\")\n",
        "    print(\"Text:\")\n",
        "    print(data['text'])\n",
        "    print('-' * 80)\n"
      ],
      "metadata": {
        "id": "J_4gLJuSeHuw",
        "outputId": "3dea4755-947f-4e18-ce34-c6711e51ff75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunks and metadata saved to document_chunks_metadata.csv\n",
            "\n",
            "Preview of first 3 chunks:\n",
            "\n",
            "Chunk ID: 1\n",
            "Number of words: 500\n",
            "Start word index: 1\n",
            "End word index: 500\n",
            "Character count: 2579\n",
            "Text:\n",
            "This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks.\n",
            "--------------------------------------------------------------------------------\n",
            "Chunk ID: 2\n",
            "Number of words: 500\n",
            "Start word index: 501\n",
            "End word index: 1000\n",
            "Character count: 2579\n",
            "Text:\n",
            "This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks.\n",
            "--------------------------------------------------------------------------------\n",
            "Chunk ID: 3\n",
            "Number of words: 500\n",
            "Start word index: 1001\n",
            "End word index: 1500\n",
            "Character count: 2579\n",
            "Text:\n",
            "This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks. This is a dummy document. It contains some sample text that will be chunked. We need enough words to make at least a few chunks.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}